# e-BrAIn.Tech - Portal de CoE de IA

Portal de integraÃ§Ã£o de IA que fornece acesso a mÃºltiplos provedores de LLM (Large Language Models), incluindo OpenAI, Anthropic (Claude), AWS Bedrock e Ollama.

## ğŸš€ CaracterÃ­sticas

- **MÃºltiplos Providers**: Suporte para OpenAI, Anthropic, AWS Bedrock e Ollama
- **ConsciÃªncia Contextual**: MantÃ©m o contexto das interaÃ§Ãµes
- **SeleÃ§Ã£o de Modelos**: Escolha entre diferentes tipos de modelos:
  - ğŸ” Code Review: Feedback detalhado sobre cÃ³digo
  - âœï¸ Text Completion: GeraÃ§Ã£o de texto coerente
  - ğŸ“ Summarization: Resumos concisos
  - ğŸ¤ Speech-to-Text: ConversÃ£o de fala em texto
  - ğŸ¨ Image Creation: GeraÃ§Ã£o de imagens baseadas em prompts
- **HistÃ³rico de InteraÃ§Ãµes**: Armazena as Ãºltimas 90 interaÃ§Ãµes
- **Interface AmigÃ¡vel**: Interface moderna e intuitiva com Streamlit
- **Arquitetura Modular**: CÃ³digo bem organizado e modular

## ğŸ“‹ PrÃ©-requisitos

- Python 3.8 ou superior
- Contas e credenciais para os providers que deseja usar:
  - OpenAI: API Key
  - Anthropic: API Key
  - AWS Bedrock: Access Key ID e Secret Access Key
  - Ollama: ServiÃ§o local (opcional)

## ğŸ”§ InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone <repository-url>
cd contax-brain
```

2. Crie um ambiente virtual:
```bash
python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
```

3. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

4. Configure as variÃ¡veis de ambiente criando um arquivo `.env` na raiz do projeto:
```env
# OpenAI (Modelos mais recentes: GPT-4o, GPT-4o-mini, GPT-4 Turbo)
OPENAI_API_KEY=sua_chave_openai_aqui
OPENAI_MODEL=gpt-4o

# Anthropic (Claude) - Modelos mais recentes: Claude 3.5 Sonnet, Claude 3.5 Haiku
ANTHROPIC_API_KEY=sua_chave_anthropic_aqui
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# AWS Bedrock (Modelos mais recentes: Claude 3.5 Sonnet, Claude 3.5 Haiku)
AWS_ACCESS_KEY_ID=seu_access_key_id_aqui
AWS_SECRET_ACCESS_KEY=seu_secret_access_key_aqui
AWS_REGION=us-east-1
AWS_BEDROCK_MODEL=anthropic.claude-3-5-sonnet-20240620-v1:0

# Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# ConfiguraÃ§Ãµes Gerais
MAX_HISTORY=90
HISTORY_FILE=history.json
```

**Nota**: VocÃª nÃ£o precisa configurar todos os providers. Configure apenas os que deseja usar.

## ğŸƒ Executando Localmente

Execute a aplicaÃ§Ã£o Streamlit:
```bash
streamlit run app.py
```

A aplicaÃ§Ã£o estarÃ¡ disponÃ­vel em `http://localhost:8501`

## â˜ï¸ Deploy no Streamlit Cloud

### Passo 1: Preparar o RepositÃ³rio

1. Certifique-se de que seu cÃ³digo estÃ¡ em um repositÃ³rio Git (GitHub, GitLab ou Bitbucket)
2. Verifique se o arquivo `requirements.txt` estÃ¡ atualizado
3. Certifique-se de que o arquivo `app.py` estÃ¡ na raiz do projeto

### Passo 2: Criar Conta no Streamlit Cloud

1. Acesse [https://streamlit.io/cloud](https://streamlit.io/cloud)
2. FaÃ§a login com sua conta GitHub/GitLab/Bitbucket
3. Autorize o Streamlit Cloud a acessar seus repositÃ³rios

### Passo 3: Deploy da AplicaÃ§Ã£o

1. No dashboard do Streamlit Cloud, clique em "New app"
2. Selecione:
   - **Repository**: Seu repositÃ³rio
   - **Branch**: Branch principal (geralmente `main` ou `master`)
   - **Main file path**: `app.py`
3. Clique em "Deploy!"

### Passo 4: Configurar VariÃ¡veis de Ambiente

ApÃ³s o deploy inicial, configure as variÃ¡veis de ambiente:

1. No dashboard do Streamlit Cloud, clique na sua aplicaÃ§Ã£o
2. VÃ¡ em "Settings" (âš™ï¸) â†’ "Secrets"
3. Adicione as variÃ¡veis de ambiente no formato TOML:

```toml
# OpenAI
OPENAI_API_KEY = "sua_chave_openai_aqui"
OPENAI_MODEL = "gpt-4"

# Anthropic
ANTHROPIC_API_KEY = "sua_chave_anthropic_aqui"
ANTHROPIC_MODEL = "claude-3-opus-20240229"

# AWS Bedrock
AWS_ACCESS_KEY_ID = "seu_access_key_id_aqui"
AWS_SECRET_ACCESS_KEY = "seu_secret_access_key_aqui"
AWS_REGION = "us-east-1"
AWS_BEDROCK_MODEL = "anthropic.claude-3-opus-20240229-v1:0"

# Ollama (geralmente nÃ£o funciona no Streamlit Cloud, apenas local)
OLLAMA_BASE_URL = "http://localhost:11434"
OLLAMA_MODEL = "llama2"

# ConfiguraÃ§Ãµes Gerais
MAX_HISTORY = "90"
HISTORY_FILE = "history.json"
```

4. Salve as configuraÃ§Ãµes
5. A aplicaÃ§Ã£o serÃ¡ reiniciada automaticamente

### Passo 5: Acessar a AplicaÃ§Ã£o

ApÃ³s o deploy, vocÃª receberÃ¡ uma URL Ãºnica para sua aplicaÃ§Ã£o, por exemplo:
`https://seu-app.streamlit.app`

## ğŸ“ Estrutura do Projeto

```
contax-brain/
â”œâ”€â”€ app.py                      # AplicaÃ§Ã£o principal Streamlit
â”œâ”€â”€ config.py                   # ConfiguraÃ§Ãµes centralizadas
â”œâ”€â”€ requirements.txt            # DependÃªncias Python
â”œâ”€â”€ README.md                   # DocumentaÃ§Ã£o
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml            # ConfiguraÃ§Ãµes do Streamlit
â”œâ”€â”€ providers/                  # MÃ³dulos de providers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                # Classe base abstrata
â”‚   â”œâ”€â”€ openai_provider.py     # Provider OpenAI
â”‚   â”œâ”€â”€ anthropic_provider.py  # Provider Anthropic
â”‚   â”œâ”€â”€ bedrock_provider.py    # Provider AWS Bedrock
â”‚   â””â”€â”€ ollama_provider.py     # Provider Ollama
â””â”€â”€ utils/                      # UtilitÃ¡rios
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ history.py             # Gerenciamento de histÃ³rico
    â””â”€â”€ provider_factory.py    # Factory de providers
```

## ğŸ”Œ Adicionando um Novo Provider

Para adicionar um novo provider de LLM:

1. Crie um novo arquivo em `providers/` (ex: `providers/novo_provider.py`)
2. Herde da classe `BaseProvider` em `providers/base.py`
3. Implemente os mÃ©todos obrigatÃ³rios:
   - `is_available()`: Verifica se o provider estÃ¡ configurado
   - `chat_completion()`: Gera respostas
   - `list_models()`: Lista modelos disponÃ­veis
4. Adicione o provider ao `ProviderFactory` em `utils/provider_factory.py`
5. Adicione as variÃ¡veis de ambiente necessÃ¡rias em `config.py`

Exemplo:
```python
from providers.base import BaseProvider, Message, ModelType

class NovoProvider(BaseProvider):
    def __init__(self):
        super().__init__("Novo Provider")
        # InicializaÃ§Ã£o
    
    def is_available(self) -> bool:
        # Verifica disponibilidade
        pass
    
    def chat_completion(self, messages, model_type, **kwargs):
        # Implementa geraÃ§Ã£o de respostas
        pass
    
    def list_models(self):
        # Lista modelos
        pass
```

## ğŸ”’ SeguranÃ§a

- **Nunca** commite arquivos `.env` ou credenciais no Git
- Use as Secrets do Streamlit Cloud para variÃ¡veis sensÃ­veis
- Mantenha suas API keys seguras e rotacione-as regularmente
- O arquivo `history.json` pode conter dados sensÃ­veis - considere criptografÃ¡-lo em produÃ§Ã£o

## ğŸ“ Notas Importantes

- **Ollama**: Funciona apenas localmente ou em servidores onde o serviÃ§o estÃ¡ rodando. NÃ£o funciona no Streamlit Cloud padrÃ£o.
- **AWS Bedrock**: Requer credenciais AWS vÃ¡lidas e acesso ao serviÃ§o Bedrock na regiÃ£o configurada.
- **HistÃ³rico**: O histÃ³rico Ã© armazenado localmente em `history.json`. No Streamlit Cloud, cada instÃ¢ncia tem seu prÃ³prio histÃ³rico.

## ğŸ¤ Contribuindo

1. FaÃ§a um fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/NovaFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona NovaFeature'`)
4. Push para a branch (`git push origin feature/NovaFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto Ã© propriedade da Twinn/ContaX.

## ğŸ†˜ Suporte

Para suporte, entre em contato com a equipe de desenvolvimento ou abra uma issue no repositÃ³rio.
